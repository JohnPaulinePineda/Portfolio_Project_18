---
title: 'R : Selecting Informative Predictors Using Univariate Filters'
author: "John Pauline Pineda"
date: "December 21, 2022"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
| This document implements univariate filters for selecting informative predictors using various helpful packages in <mark style="background-color: #CCECFF">**R**</mark>.    
|
##  1.1 Sample Data
|
| The <mark style="background-color: #EEEEEE;color: #FF0000">**AlzheimerDisease**</mark>  dataset from the  <mark style="background-color: #CCECFF">**AppliedPredictiveModeling**</mark> package was used for this illustrated example.
|
| Preliminary dataset assessment:
|
| **[A]** 333 rows (observations)
|      **[A.1]** Train Set = 267 observations
|      **[A.2]** Test Set = 66 observations
| 
| **[B]** 128 columns (variables)
|      **[B.1]** 1/128 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=Control</span> < <span style="color: #FF0000">Class=Impaired</span>
|      **[B.2]** 127/128 predictors = All remaining variables (3/127 factor + 124/127 numeric)
|     
| 
```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(lattice)
library(dplyr)
library(tidyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(tidyverse)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stats)
library(nnet)
library(elasticnet)
library(earth)
library(party)
library(kernlab)
library(randomForest)
library(Cubist)
library(pROC)
library(mda)
library(klaR)
library(pamr)

##################################
# Loading source and
# formulating the train set
##################################
data(AlzheimerDisease)
Alzheimer <- predictors
Alzheimer$Class <- diagnosis

##################################
# Decomposing the Genotype factor
# into binary dummy variables
##################################

## Decompose the genotype factor into binary dummy variables

Alzheimer$E2 <- Alzheimer$E3 <- Alzheimer$E4 <- 0
Alzheimer$E2[grepl("2", Alzheimer$Genotype)] <- 1
Alzheimer$E3[grepl("3", Alzheimer$Genotype)] <- 1
Alzheimer$E4[grepl("4", Alzheimer$Genotype)] <- 1
Alzheimer_Original <- Alzheimer

##################################
# Removing baseline predictors
##################################
Alzheimer <- Alzheimer[,!(names(Alzheimer) %in% c("Genotype", "age", "tau", "p_tau", "Ab_42", "male"))]

##################################
# Partitoning the data into
# train and test sets
##################################
set.seed(12345678)
Alzheimer_Train_Index <- createDataPartition(Alzheimer$Class,p=0.8)[[1]]
Alzheimer_Train <- Alzheimer[ Alzheimer_Train_Index, ]
Alzheimer_Test  <- Alzheimer[-Alzheimer_Train_Index, ]

##################################
# Performing a general exploration of the train set
##################################
dim(Alzheimer_Train)
str(Alzheimer_Train)
summary(Alzheimer_Train)

##################################
# Performing a general exploration of the test set
##################################
dim(Alzheimer_Test)
str(Alzheimer_Test)
summary(Alzheimer_Test)

##################################
# Formulating a data type assessment summary
##################################
PDA <- Alzheimer_Train
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)
```
##  1.2 Data Quality Assessment
|
| Data quality assessment:
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** Low variance observed for 2 variables with First.Second.Mode.Ratio>5.
|      **[B.1]** <span style="color: #FF0000">E2</span> variable (factor)
|      **[B.2]** <span style="color: #FF0000">E3</span> variable (factor)
|
| **[C]** No low variance observed for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** No high skewness observed for any variable with Skewness>3 or Skewness<(-3).
|
```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- Alzheimer_Train

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Index=c(1:length(names(DQA))),
  Column.Name= names(DQA), 
  Column.Type=sapply(DQA, function(x) class(x)), 
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all predictors
##################################
DQA.Predictors <- DQA[,!names(DQA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DQA.Predictors.Numeric <- DQA.Predictors[,!names(DQA.Predictors) %in% c("E2","E3","E4")]
DQA.Predictors.Numeric <- as.data.frame(sapply(DQA.Predictors.Numeric,function(x) as.numeric(x)))

if (length(names(DQA.Predictors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Numeric))),
               " numeric predictor variable(s)."))
} else {
  print("There are no numeric predictor variables.")
}

##################################
# Listing all factor predictors
##################################
DQA.Predictors.Factor <- DQA.Predictors[,names(DQA.Predictors) %in% c("E2","E3","E4")]
DQA.Predictors.Factor <- as.data.frame(sapply(DQA.Predictors.Factor,function(x) as.factor(x)))

if (length(names(DQA.Predictors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Factor))),
               " factor predictor variable(s)."))
} else {
  print("There are no factor predictor variables.")
}

##################################
# Formulating a data quality assessment summary for factor predictors
##################################
if (length(names(DQA.Predictors.Factor))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Factor), 
  Column.Type=sapply(DQA.Predictors.Factor, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )
  
} 

##################################
# Formulating a data quality assessment summary for numeric predictors
##################################
if (length(names(DQA.Predictors.Numeric))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Numeric), 
  Column.Type=sapply(DQA.Predictors.Numeric, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Predictors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Predictors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Predictors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Predictors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Predictors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Predictors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )  
  
}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance predictors
##################################
if (length(names(DQA.Predictors.Factor))==0) {
  print("No factor predictors noted.")
} else if (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric predictors due to low unique count ratio noted.")
}

##################################
# Checking for skewed predictors
##################################
if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric predictors noted.")
}

```
##  1.3 Data Preprocessing

###  1.3.1 Outlier
|
| Outlier data assessment:
|
| **[A]** Few outliers noted for most variables  with the numeric data visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile). Outlier treatment for numerical stability remains optional depending on potential model requirements for the subsequent steps. 6 variables were observed with at least 10 outliers listed as follows:
|      **[A.1]** <span style="color: #FF0000">Apolipoprotein_CI</span> variable (10 outliers detected)
|      **[A.2]** <span style="color: #FF0000">Cortisol</span> variable (11 outliers detected)
|      **[A.3]** <span style="color: #FF0000">IL_17E</span> variable (11 outliers detected)
|      **[A.4]** <span style="color: #FF0000">IL6</span> variable (19 outliers detected)
|      **[A.5]** <span style="color: #FF0000">MCP_2</span> variable (21 outliers detected)
|      **[A.6]** <span style="color: #FF0000">Prostatic_Acid_Phospatase</span> variable (10 outliers detected)
|
```{r section_1.3.1, warning=FALSE, message=FALSE, fig.width=15, fig.height=3}
##################################
# Loading dataset
##################################
DPA <- Alzheimer_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,!names(DPA.Predictors) %in% c("E2","E3","E4")]
DPA.Predictors.Numeric <- as.data.frame(sapply(DPA.Predictors.Numeric,function(x) as.numeric(x)))

##################################
# Identifying outliers for the numeric predictors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Predictors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Predictors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Predictors.Numeric[,i] %in% c(Outliers))
  boxplot(DPA.Predictors.Numeric[,i], 
          ylab = names(DPA.Predictors.Numeric)[i], 
          main = names(DPA.Predictors.Numeric)[i],
          horizontal=TRUE)
  mtext(paste0(OutlierCount, " Outlier(s) Detected"))
}

OutlierCountSummary <- as.data.frame(cbind(names(DPA.Predictors.Numeric),(OutlierCountList)))
names(OutlierCountSummary) <- c("NumericPredictors","OutlierCount")
OutlierCountSummary$OutlierCount <- as.numeric(as.character(OutlierCountSummary$OutlierCount))
NumericPredictorWithOutlierCount <- nrow(OutlierCountSummary[OutlierCountSummary$OutlierCount>0,])
print(paste0(NumericPredictorWithOutlierCount, " numeric variable(s) were noted with outlier(s)." ))

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA.Predictors.Numeric))

###################################
# Verifying the data dimensions
###################################
dim(DPA.Predictors.Numeric)

```

###  1.3.2 Zero and Near-Zero Variance
|
| Zero and near-zero variance data assessment:
|
| **[A]** Low variance noted for 2 variables from the previous data quality assessment using a lower threshold.
|
| **[B]** No low variance noted for any variable using a preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package. The <span style="color: #0000FF">nearZeroVar</span> method using both the <span style="color: #0000FF">freqCut</span> and <span style="color: #0000FF">uniqueCut</span> criteria set at 95/5 and 10, respectively, were applied on the dataset.
|
```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- DPA.Predictors

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 95/5,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){
  
  print("No low variance predictors noted.")
  
} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))
  
  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))
  
  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }
  
  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

  ##################################
  # Filtering out columns with low variance
  #################################
  DPA_ExcludedLowVariance <- DPA[,!names(DPA) %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,])]
  
  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLowVariance_Skimmed <- skim(DPA_ExcludedLowVariance))
}

```

###  1.3.3 Collinearity
|
| High collinearity data assessment:
|
| **[A]** No high correlation > 95% were noted for any variable pair as confirmed using the preprocessing summaries from the <mark style="background-color: #CCECFF">**caret**</mark> and <mark style="background-color: #CCECFF">**lares**</mark> packages.
|
```{r section_1.3.3, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Alzheimer_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,!names(DPA.Predictors) %in% c("E2","E3","E4")]
DPA.Predictors.Numeric <- as.data.frame(sapply(DPA.Predictors.Numeric,function(x) as.numeric(x)))

##################################
# Visualizing pairwise correlation between predictors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Predictors.Numeric,
                       method = "pearson",
                       conf.level = .95)

corrplot(cor(DPA.Predictors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"), 
         method = "circle",
         type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.cex = 0.75,
         tl.srt = 90, 
         sig.level = 0.05, 
         p.mat = DPA_CorrelationTest$p,
         insig = "blank")



##################################
# Identifying the highly correlated variables
##################################
DPA_Correlation <-  cor(DPA.Predictors.Numeric, 
                        method = "pearson",
                        use="pairwise.complete.obs")
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)]) > 0.95))

if (DPA_HighlyCorrelatedCount == 0) {
  print("No highly correlated predictors noted.")
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.95."))
  
  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Predictors.Numeric,
  max_pvalue = 0.05, 
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))
  
}


if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.95)
  
  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))
  
  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }
  
  ##################################
  # Filtering out columns with high correlation
  #################################
  DPA_ExcludedHighCorrelation <- DPA[,-DPA_HighlyCorrelated]
  
  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedHighCorrelation_Skimmed <- skim(DPA_ExcludedHighCorrelation))

}

```

###  1.3.4 Linear Dependencies
|
| Linear dependency data assessment:
|
| **[A]** No linear dependencies noted for any subset of variables using the preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package applying the <span style="color: #0000FF">findLinearCombos</span> method which utilizes the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). 
|
```{r section_1.3.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Alzheimer_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,!names(DPA.Predictors) %in% c("E2","E3","E4")]
DPA.Predictors.Numeric <- as.data.frame(sapply(DPA.Predictors.Numeric,function(x) as.numeric(x)))

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))
  
  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }
  
}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)
  
  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)
  
  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }
  
  ##################################
  # Filtering out columns with linear dependency
  #################################
  DPA_ExcludedLinearlyDependent <- DPA[,-DPA_LinearlyDependent$remove]
  
  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLinearlyDependent_Skimmed <- skim(DPA_ExcludedLinearlyDependent))

}

```

###  1.3.5 Pre-Processed Dataset
| Preliminary dataset assessment:
|
| **[A]** 333 rows (observations)
|      **[A.1]** Train Set = 267 observations
|      **[A.2]** Test Set = 66 observations
| 
| **[B]** 128 columns (variables)
|      **[B.1]** 1/128 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=Control</span> < <span style="color: #FF0000">Class=Impaired</span>
|      **[B.2]** 127/128 predictors = All remaining variables (3/127 factor + 124/127 numeric)
|
| **[C]** No pre-processing actions applied:
|      **[C.1]** No shape transformation applied since distributions were fairly normal
|      **[C.2]** Centering and scaling may be necessary due to the differences in ranges for the numeric variables however, these will be selectively applied based on model requirements
|      **[C.2]** No outlier treatment applied since the high values noted were minimal and contextually valid
|      **[C.3]** No predictors removed due to zero or near-zero variance 
|      **[C.4]** No predictors removed due to high correlation
|      **[C.5]** No predictors removed due to linear dependencies
|
```{r section_1.3.7, warning=FALSE, message=FALSE}
##################################
# Creating the pre-modelling
# train set
##################################
PMA_PreModelling_Train <- Alzheimer_Train
PMA_PreModelling_Train$Class <- as.factor(PMA_PreModelling_Train$Class)
PMA_PreModelling_Train$E2 <- as.factor(PMA_PreModelling_Train$E2)
PMA_PreModelling_Train$E3 <- as.factor(PMA_PreModelling_Train$E3)
PMA_PreModelling_Train$E4 <- as.factor(PMA_PreModelling_Train$E4)

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Train_Skimmed <- skim(PMA_PreModelling_Train))

###################################
# Verifying the data dimensions
# for the train set
###################################
dim(PMA_PreModelling_Train)

##################################
# Formulating the test set
##################################
PMA_PreModelling_Test <- Alzheimer_Test
PMA_PreModelling_Test$Class <- as.factor(PMA_PreModelling_Test$Class)
PMA_PreModelling_Test$E2 <- as.factor(PMA_PreModelling_Test$E2)
PMA_PreModelling_Test$E3 <- as.factor(PMA_PreModelling_Test$E3)
PMA_PreModelling_Test$E4 <- as.factor(PMA_PreModelling_Test$E4)

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Test_Skimmed <- skim(PMA_PreModelling_Test))

###################################
# Verifying the data dimensions
# for the test set
###################################
dim(PMA_PreModelling_Test)

```

##  1.4 Data Exploration
|
| Exploratory data analysis:
|
| **[A]** Numeric variables which demonstrated differential relationships with the <span style="color: #FF0000">Class</span> response variable include:
|      **[A.1]** <span style="color: #FF0000">Fibrinogen</span> variable (numeric)
|      **[A.2]** <span style="color: #FF0000">GRO_alpha</span> variable (numeric)
|      **[A.3]** <span style="color: #FF0000">FAS</span> variable (numeric)
|      **[A.4]** <span style="color: #FF0000">Eotaxin_3</span> variable (numeric)
|      **[A.5]** <span style="color: #FF0000">Creatine_Kinase_MB</span> variable (numeric)
|      **[A.6]** <span style="color: #FF0000">IGF_BP2</span> variable (numeric)
|      **[A.7]** <span style="color: #FF0000">Gamma_Interferon_Induced_Monokin</span> variable (numeric)
|      **[A.8]** <span style="color: #FF0000">MCP_2</span> variable (numeric)
|      **[A.9]** <span style="color: #FF0000">MIF</span> variable (numeric)
|      **[A.10]** <span style="color: #FF0000">Pancreatic_polypteptide</span> variable (numeric)
|      **[A.11]** <span style="color: #FF0000">PAI_1</span> variable (numeric)
|      **[A.12]** <span style="color: #FF0000">NT_proBNP</span> variable (numeric)
|      **[A.13]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|      **[A.14]** <span style="color: #FF0000">MMP7</span> variable (numeric)
|      **[A.15]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|      **[A.16]** <span style="color: #FF0000">Pulmonary_and_Activation_Regulat</span> variable (numeric)
|      **[A.17]** <span style="color: #FF0000">Resistin</span> variable (numeric)
|      **[A.18]** <span style="color: #FF0000">VEGF</span> variable (numeric)
|      **[A.19]** <span style="color: #FF0000">Thrombopoietin</span> variable (numeric)
|      **[A.20]** <span style="color: #FF0000">Thymus_Expressed_Chemokine_TECK</span> variable (numeric)
|
| **[B]** Factor variables which demonstrated relatively better differentiation of the <span style="color: #FF0000">Class</span> response variable between its <span style="color: #FF0000">Control</span> and <span style="color: #FF0000">Impaired</span> levels include:
|      **[B.1]** <span style="color: #FF0000">E2</span> variable (factor)
|      **[B.2]** <span style="color: #FF0000">E4</span> variable (factor)
|
```{r section_1.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
EDA <- PMA_PreModelling_Train

##################################
# Listing all predictors
##################################
EDA.Predictors <- EDA[,!names(EDA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
EDA.Predictors.Numeric <- EDA.Predictors[,sapply(EDA.Predictors, is.numeric)]
ncol(EDA.Predictors.Numeric)
names(EDA.Predictors.Numeric)

##################################
# Listing all factor predictors
##################################
EDA.Predictors.Factor <- EDA.Predictors[,sapply(EDA.Predictors, is.factor)]
ncol(EDA.Predictors.Factor)
names(EDA.Predictors.Factor)

##################################
# Formulating the box plots
##################################
featurePlot(x = EDA.Predictors.Numeric[1:124], 
            y = EDA$Class,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90), 
                          y = list(relation="free")),
            adjust = 1.5, 
            pch = "|",
            layout=(c(4,4)))

##################################
# Restructuring the dataset for
# for barchart analysis
##################################
EDA.Bar.Source <- as.data.frame(cbind(EDA$Class,
                                      EDA.Predictors.Factor))
names(EDA.Bar.Source) <- c("Class",names(EDA.Predictors.Factor))
ncol(EDA.Bar.Source)

##################################
# Creating a function to formulate
# the proportions table
##################################
EDA.PropTable.Function <- function(FactorVar) {
  EDA.Bar.Source.FactorVar <- EDA.Bar.Source[,c("Class",
                                                FactorVar)]
  EDA.Bar.Source.FactorVar.Prop <- as.data.frame(prop.table(table(EDA.Bar.Source.FactorVar), 2))
  names(EDA.Bar.Source.FactorVar.Prop)[2] <- "Class"
  EDA.Bar.Source.FactorVar.Prop$Variable <- rep(FactorVar,nrow(EDA.Bar.Source.FactorVar.Prop))

  return(EDA.Bar.Source.FactorVar.Prop)

}

EDA.Bar.Source.FactorVar.Prop <- rbind(EDA.PropTable.Function("E2"),
                                       EDA.PropTable.Function("E3"),
                                       EDA.PropTable.Function("E4"))

(EDA.Barchart.FactorVar <- barchart(EDA.Bar.Source.FactorVar.Prop[,3] ~
                                      EDA.Bar.Source.FactorVar.Prop[,2] | EDA.Bar.Source.FactorVar.Prop[,4],
                                      data=EDA.Bar.Source.FactorVar.Prop,
                                      groups = EDA.Bar.Source.FactorVar.Prop[,1],
                                      stack=TRUE,
                                      ylab = "Proportion",
                                      xlab = "Class",
                                      auto.key = list(adj = 1),
                                      layout=(c(3,1))))

```

##  1.5 Univariate Filters (UF)

###  1.5.1 Linear Discriminant Analysis Without UF (LDA_FULL)
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented without recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[C.2]** ROC Curve AUC = 0.80151
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.77199
|
```{r section_1.5.1, warning=FALSE, message=FALSE}
##################################
# Converting all predictors to numeric
# for both train and test data
##################################
for (i in 1:ncol(PMA_PreModelling_Train)){
  if (names(PMA_PreModelling_Train)[i]!="Class"){
    PMA_PreModelling_Train[,i] <- as.numeric(PMA_PreModelling_Train[,i])
  }
}
summary(PMA_PreModelling_Train)

for (i in 1:ncol(PMA_PreModelling_Test)){
  if (names(PMA_PreModelling_Test)[i]!="Class"){
    PMA_PreModelling_Test[,i] <- as.numeric(PMA_PreModelling_Test[,i])
  }
}
summary(PMA_PreModelling_Test)

##################################
# Creating consistent fold assignments 
# for the Cross Validation process
##################################
set.seed(12345678)
KFold_Indices <- createFolds(PMA_PreModelling_Train$Class ,
                             k = 10,
                             returnTrain=TRUE)

##################################
# Formulating a function to summarize
# model performance metrics
##################################
FiveMetricsSummary <- function(...) c(twoClassSummary(...), defaultSummary(...))

##################################
# Formulating the controls for the 
# model training process
##################################
KFold_TrainControl <- trainControl(method = "cv",
                                   summaryFunction = FiveMetricsSummary,
                                   classProbs = TRUE,
                                   index = KFold_Indices)

##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
##################################
set.seed(12345678)
LDA_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                         y = PMA_PreModelling_Train$Class,
                         method = "lda",
                         metric = "ROC",
                         tol = 1.0e-12,
                         trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_FULL_Tune

LDA_FULL_Tune$finalModel

LDA_FULL_Tune$results

(LDA_FULL_Train_ROCCurveAUC <- LDA_FULL_Tune$results[,c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_FULL_Test <- data.frame(LDA_FULL_Observed = PMA_PreModelling_Test$Class,
                      LDA_FULL_Predicted = predict(LDA_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

LDA_FULL_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
LDA_FULL_Test_ROC <- roc(response = LDA_FULL_Test$LDA_FULL_Observed,
                        predictor = LDA_FULL_Test$LDA_FULL_Predicted.Impaired,
                        levels = rev(levels(LDA_FULL_Test$LDA_FULL_Observed)))

(LDA_FULL_Test_ROCCurveAUC <- auc(LDA_FULL_Test_ROC)[1])

```

###  1.5.2 Linear Discriminant Analysis With UF Using No Adjustment and Correlated Predictors (LDA_UF_NAC)
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with univariate filters using no adjustment for the computed p-values and correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 58 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 54 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=47 to 62
|      **[D.2]** ROC Curve AUC = 0.84378
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.88310
|
```{r section_1.5.2, warning=FALSE, message=FALSE}
##################################
# Creating a function to calculates p-values
# using either a t-test for predictors with 
# more than 2 distinct values
# using Fisher's Exact Test otherwise
##################################
PScore <- function(x, y){
    numX <- length(unique(x))
    if(numX > 2)
      {
       out <- t.test(x ~ y)$p.value
      } else {
       out <- fisher.test(factor(x), y)$p.value
      }
    out
}

LDAPValue <- ldaSBF
LDAPValue$score <- PScore
LDAPValue$summary <- FiveMetricsSummary

##################################
# Creating a function to filter out
# predictors with p-values greater than 0.05
##################################
LDAPValue$filter <- function (Score, x, y){
  InformativePredictors <- Score <= 0.05
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = LDAPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
# with implementation of univariate filter
##################################
set.seed(12345678)
LDA_UF_NAC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              tol = 1.0e-12,
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_UF_NAC_Tune

LDA_UF_NAC_Tune$fit

LDA_UF_NAC_Tune$results

(LDA_UF_NAC_Train_ROCCurveAUC <- LDA_UF_NAC_Tune$results[LDA_UF_NAC_Tune$results$ROC==max(LDA_UF_NAC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_UF_NAC_Test <- data.frame(LDA_UF_NAC_Observed = PMA_PreModelling_Test$Class,
                              LDA_UF_NAC_Predicted = predict(LDA_UF_NAC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

LDA_UF_NAC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
LDA_UF_NAC_Test_ROC <- roc(response = LDA_UF_NAC_Test$LDA_UF_NAC_Observed,
                           predictor = LDA_UF_NAC_Test$LDA_UF_NAC_Predicted.Impaired,
                           levels = rev(levels(LDA_UF_NAC_Test$LDA_UF_NAC_Observed)))

(LDA_UF_NAC_Test_ROCCurveAUC <- auc(LDA_UF_NAC_Test_ROC)[1])

```

###  1.5.3 Linear Discriminant Analysis With UF Using Bonferroni Adjustment and Correlated Predictors (LDA_UF_BAC)
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with univariate filters using Bonferroni adjustment for the computed p-values and correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 15 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 13 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=10 to 18
|      **[D.2]** ROC Curve AUC = 0.75914
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.80324
|
```{r section_1.5.3, warning=FALSE, message=FALSE}
##################################
# Creating a function to filter out
# predictors with bonferroni-adjusted 
# p-values greater than 0.05
##################################
LDAPValue$filter <- function (Score, x, y){
  Score <- p.adjust(Score,  "bonferroni")
  InformativePredictors <- Score <= 0.05
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = LDAPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
# with implementation of univariate filter
##################################
set.seed(12345678)
LDA_UF_BAC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              tol = 1.0e-12,
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_UF_BAC_Tune

LDA_UF_BAC_Tune$fit

LDA_UF_BAC_Tune$results

(LDA_UF_BAC_Train_ROCCurveAUC <- LDA_UF_BAC_Tune$results[LDA_UF_BAC_Tune$results$ROC==max(LDA_UF_BAC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_UF_BAC_Test <- data.frame(LDA_UF_BAC_Observed = PMA_PreModelling_Test$Class,
                              LDA_UF_BAC_Predicted = predict(LDA_UF_BAC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

LDA_UF_BAC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
LDA_UF_BAC_Test_ROC <- roc(response = LDA_UF_BAC_Test$LDA_UF_BAC_Observed,
                           predictor = LDA_UF_BAC_Test$LDA_UF_BAC_Predicted.Impaired,
                           levels = rev(levels(LDA_UF_BAC_Test$LDA_UF_BAC_Observed)))

(LDA_UF_BAC_Test_ROCCurveAUC <- auc(LDA_UF_BAC_Test_ROC)[1])

```

###  1.5.4 Linear Discriminant Analysis With UF Using No Adjustment and No Correlated Predictors (LDA_UF_NANC)
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with univariate filters using no adjustment for the computed p-values and no correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 54 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 52 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=45 to 59
|      **[D.2]** ROC Curve AUC = 0.84130
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.88657
|
```{r section_1.5.4, warning=FALSE, message=FALSE}
##################################
# Creating a function to filter out
# predictors with p-values greater than 0.05
##################################
LDAPValue$filter <- function (Score, x, y){
  InformativePredictors <- Score <= 0.05
  CorrelationMatrix <- cor(x[,InformativePredictors])
  HighlyCorrelated <- findCorrelation(CorrelationMatrix, 0.75)
  if(length(HighlyCorrelated)>0) InformativePredictors[HighlyCorrelated] <- FALSE
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = LDAPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
# with implementation of univariate filter
##################################
set.seed(12345678)
LDA_UF_NANC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              tol = 1.0e-12,
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_UF_NANC_Tune

LDA_UF_NANC_Tune$fit

LDA_UF_NANC_Tune$results

(LDA_UF_NANC_Train_ROCCurveAUC <- LDA_UF_NANC_Tune$results[LDA_UF_NANC_Tune$results$ROC==max(LDA_UF_NANC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_UF_NANC_Test <- data.frame(LDA_UF_NANC_Observed = PMA_PreModelling_Test$Class,
                              LDA_UF_NANC_Predicted = predict(LDA_UF_NANC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

LDA_UF_NANC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
LDA_UF_NANC_Test_ROC <- roc(response = LDA_UF_NANC_Test$LDA_UF_NANC_Observed,
                           predictor = LDA_UF_NANC_Test$LDA_UF_NANC_Predicted.Impaired,
                           levels = rev(levels(LDA_UF_NANC_Test$LDA_UF_NANC_Observed)))

(LDA_UF_NANC_Test_ROCCurveAUC <- auc(LDA_UF_NANC_Test_ROC)[1])

```

###  1.5.5 Linear Discriminant Analysis With UF Using Bonferroni Adjustment and No Correlated Predictors (LDA_UF_BANC)
|
| **[A]** The linear discriminant analysis model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with univariate filters using Bonferroni adjustment for the computed p-values and no correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 15 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 13 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=10 to 18
|      **[D.2]** ROC Curve AUC = 0.75914
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.80324
|
```{r section_1.5.5, warning=FALSE, message=FALSE}
##################################
# Creating a function to filter out
# predictors with p-values greater than 0.05
##################################
LDAPValue$filter <- function (Score, x, y){
  Score <- p.adjust(Score,  "bonferroni")
  InformativePredictors <- Score <= 0.05
  CorrelationMatrix <- cor(x[,InformativePredictors])
  HighlyCorrelated <- findCorrelation(CorrelationMatrix, 0.75)
  if(length(HighlyCorrelated)>0) InformativePredictors[HighlyCorrelated] <- FALSE
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = LDAPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the linear discriminant analysis model
# by setting the caret method to 'lda'
# with implementation of univariate filter
##################################
set.seed(12345678)
LDA_UF_BANC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              tol = 1.0e-12,
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
LDA_UF_BANC_Tune

LDA_UF_BANC_Tune$fit

LDA_UF_BANC_Tune$results

(LDA_UF_BANC_Train_ROCCurveAUC <- LDA_UF_BANC_Tune$results[LDA_UF_BANC_Tune$results$ROC==max(LDA_UF_BANC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
LDA_UF_BANC_Test <- data.frame(LDA_UF_BANC_Observed = PMA_PreModelling_Test$Class,
                              LDA_UF_BANC_Predicted = predict(LDA_UF_BANC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

LDA_UF_BANC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
LDA_UF_BANC_Test_ROC <- roc(response = LDA_UF_BANC_Test$LDA_UF_BANC_Observed,
                           predictor = LDA_UF_BANC_Test$LDA_UF_BANC_Predicted.Impaired,
                           levels = rev(levels(LDA_UF_BANC_Test$LDA_UF_BANC_Observed)))

(LDA_UF_BANC_Test_ROCCurveAUC <- auc(LDA_UF_BANC_Test_ROC)[1])

```

###  1.5.6 Random Forest Without UF (RF_FULL)
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**randomForest**</mark> package was implemented without recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 11
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration involves mtry=11
|      **[C.2]** ROC Curve AUC = 0.78268
|
| **[D]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[D.1]** <span style="color: #FF0000">MMP10</span> variable (numeric)
|      **[D.2]** <span style="color: #FF0000">Crystatin_C</span> variable (numeric)
|      **[D.3]** <span style="color: #FF0000">MMP7</span> variable (numeric)
|      **[D.4]** <span style="color: #FF0000">TRAIL_R3</span> variable (numeric)
|      **[D.5]** <span style="color: #FF0000">Pancreatic_polypeptide</span> variable (numeric)
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.79803
|
```{r section_1.5.6, warning=FALSE, message=FALSE}
##################################
# Running the random forest model
# by setting the caret method to 'rf'
##################################
set.seed(12345678)
RF_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                        y = PMA_PreModelling_Train$Class,
                        method = "rf",
                        metric = "ROC",
                        tuneGrid = data.frame(mtry = floor(sqrt(length(names(PMA_PreModelling_Train) %in% c("Class"))))),
                        ntree = 100,
                        trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_FULL_Tune

RF_FULL_Tune$finalModel

RF_FULL_Tune$results

(RF_FULL_Train_ROCCurveAUC <- RF_FULL_Tune$results[,c("ROC")])

##################################
# Identifying and plotting the
# best model predictors
##################################
RF_FULL_VarImp <- varImp(RF_FULL_Tune, scale = TRUE)
plot(RF_FULL_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Random Forest",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_FULL_Test <- data.frame(RF_FULL_Observed = PMA_PreModelling_Test$Class,
                      RF_FULL_Predicted = predict(RF_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

RF_FULL_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RF_FULL_Test_ROC <- roc(response = RF_FULL_Test$RF_FULL_Observed,
                        predictor = RF_FULL_Test$RF_FULL_Predicted.Impaired,
                        levels = rev(levels(RF_FULL_Test$RF_FULL_Observed)))

(RF_FULL_Test_ROCCurveAUC <- auc(RF_FULL_Test_ROC)[1])

```


###  1.5.7 Random Forest With UF Using No Adjustment and Correlated Predictors (RF_UF_NAC)
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with univariate filters using no adjustment for the computed p-values and correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 7
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 58 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 54 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=47 to 62
|      **[D.2]** ROC Curve AUC = 0.81563
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.81944
|
```{r section_1.5.7, warning=FALSE, message=FALSE}
##################################
# Creating a function to calculates p-values
# using either a t-test for predictors with 
# more than 2 distinct values
# using Fisher's Exact Test otherwise
##################################
PScore <- function(x, y){
    numX <- length(unique(x))
    if(numX > 2)
      {
       out <- t.test(x ~ y)$p.value
      } else {
       out <- fisher.test(factor(x), y)$p.value
      }
    out
}

RFPValue <- rfSBF
RFPValue$score <- PScore
RFPValue$summary <- FiveMetricsSummary

##################################
# Creating a function to filter out
# predictors with p-values greater than 0.05
##################################
RFPValue$filter <- function (Score, x, y){
  InformativePredictors <- Score <= 0.05
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = RFPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the random forest model
# by setting the caret method to 'rf'
# with implementation of univariate filter
##################################
set.seed(12345678)
RF_UF_NAC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              ntree = 100,
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_UF_NAC_Tune

RF_UF_NAC_Tune$fit

RF_UF_NAC_Tune$results

(RF_UF_NAC_Train_ROCCurveAUC <- RF_UF_NAC_Tune$results[RF_UF_NAC_Tune$results$ROC==max(RF_UF_NAC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_UF_NAC_Test <- data.frame(RF_UF_NAC_Observed = PMA_PreModelling_Test$Class,
                              RF_UF_NAC_Predicted = predict(RF_UF_NAC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

RF_UF_NAC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RF_UF_NAC_Test_ROC <- roc(response = RF_UF_NAC_Test$RF_UF_NAC_Observed,
                           predictor = RF_UF_NAC_Test$RF_UF_NAC_Predicted.Impaired,
                           levels = rev(levels(RF_UF_NAC_Test$RF_UF_NAC_Observed)))

(RF_UF_NAC_Test_ROCCurveAUC <- auc(RF_UF_NAC_Test_ROC)[1])

```

###  1.5.8 Random Forest With UF Using Bonferroni Adjustment and Correlated Predictors (RF_UF_BAC)
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with univariate filters using Bonferroni adjustment for the computed p-values and correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 3
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 15 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 13 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=10 to 18
|      **[D.2]** ROC Curve AUC = 0.75888
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.75868
|
```{r section_1.5.8, warning=FALSE, message=FALSE}
##################################
# Creating a function to filter out
# predictors with bonferroni-adjusted 
# p-values greater than 0.05
##################################
RFPValue$filter <- function (Score, x, y){
  Score <- p.adjust(Score,  "bonferroni")
  InformativePredictors <- Score <= 0.05
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = RFPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the random forest model
# by setting the caret method to 'rf'
# with implementation of univariate filter
##################################
set.seed(12345678)
RF_UF_BAC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              ntree = 100,
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_UF_BAC_Tune

RF_UF_BAC_Tune$fit

RF_UF_BAC_Tune$results

(RF_UF_BAC_Train_ROCCurveAUC <- RF_UF_BAC_Tune$results[RF_UF_BAC_Tune$results$ROC==max(RF_UF_BAC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_UF_BAC_Test <- data.frame(RF_UF_BAC_Observed = PMA_PreModelling_Test$Class,
                              RF_UF_BAC_Predicted = predict(RF_UF_BAC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

RF_UF_BAC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RF_UF_BAC_Test_ROC <- roc(response = RF_UF_BAC_Test$RF_UF_BAC_Observed,
                           predictor = RF_UF_BAC_Test$RF_UF_BAC_Predicted.Impaired,
                           levels = rev(levels(RF_UF_BAC_Test$RF_UF_BAC_Observed)))

(RF_UF_BAC_Test_ROCCurveAUC <- auc(RF_UF_BAC_Test_ROC)[1])

```

###  1.5.9 Random Forest With UF Using No Adjustment and No Correlated Predictors (RF_UF_NANC)
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**MASS**</mark> package was implemented with univariate filters using no adjustment for the computed p-values and no correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 7
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 54 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 52 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=45 to 59
|      **[D.2]** ROC Curve AUC = 0.81301
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.82002
|
```{r section_1.5.9, warning=FALSE, message=FALSE}
##################################
# Creating a function to filter out
# predictors with p-values greater than 0.05
##################################
RFPValue$filter <- function (Score, x, y){
  InformativePredictors <- Score <= 0.05
  CorrelationMatrix <- cor(x[,InformativePredictors])
  HighlyCorrelated <- findCorrelation(CorrelationMatrix, 0.75)
  if(length(HighlyCorrelated)>0) InformativePredictors[HighlyCorrelated] <- FALSE
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = RFPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the random forest model
# by setting the caret method to 'rf'
# with implementation of univariate filter
##################################
set.seed(12345678)
RF_UF_NANC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              ntree = 100,
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_UF_NANC_Tune

RF_UF_NANC_Tune$fit

RF_UF_NANC_Tune$results

(RF_UF_NANC_Train_ROCCurveAUC <- RF_UF_NANC_Tune$results[RF_UF_NANC_Tune$results$ROC==max(RF_UF_NANC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_UF_NANC_Test <- data.frame(RF_UF_NANC_Observed = PMA_PreModelling_Test$Class,
                              RF_UF_NANC_Predicted = predict(RF_UF_NANC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

RF_UF_NANC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RF_UF_NANC_Test_ROC <- roc(response = RF_UF_NANC_Test$RF_UF_NANC_Observed,
                           predictor = RF_UF_NANC_Test$RF_UF_NANC_Predicted.Impaired,
                           levels = rev(levels(RF_UF_NANC_Test$RF_UF_NANC_Observed)))

(RF_UF_NANC_Test_ROCCurveAUC <- auc(RF_UF_NANC_Test_ROC)[1])

```

###  1.5.10 Random Forest With UF Using Bonferroni Adjustment and No Correlated Predictors (RF_UF_BANC)
|
| **[A]** The random forest model from the  <mark style="background-color: #CCECFF">**randomForest**</mark> package was implemented with univariate filters using Bonferroni adjustment for the computed p-values and no correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">mtry</span> = number of randomly selected predictors held constant at a value of 3
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 15 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 13 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=10 to 18
|      **[D.2]** ROC Curve AUC = 0.75888
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.75868
|
```{r section_1.5.10, warning=FALSE, message=FALSE}
##################################
# Creating a function to filter out
# predictors with p-values greater than 0.05
##################################
RFPValue$filter <- function (Score, x, y){
  Score <- p.adjust(Score,  "bonferroni")
  InformativePredictors <- Score <= 0.05
  CorrelationMatrix <- cor(x[,InformativePredictors])
  HighlyCorrelated <- findCorrelation(CorrelationMatrix, 0.75)
  if(length(HighlyCorrelated)>0) InformativePredictors[HighlyCorrelated] <- FALSE
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = RFPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the random forest model
# by setting the caret method to 'rf'
# with implementation of univariate filter
##################################
set.seed(12345678)
RF_UF_BANC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              ntree = 100,
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
RF_UF_BANC_Tune

RF_UF_BANC_Tune$fit

RF_UF_BANC_Tune$results

(RF_UF_BANC_Train_ROCCurveAUC <- RF_UF_BANC_Tune$results[RF_UF_BANC_Tune$results$ROC==max(RF_UF_BANC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
RF_UF_BANC_Test <- data.frame(RF_UF_BANC_Observed = PMA_PreModelling_Test$Class,
                              RF_UF_BANC_Predicted = predict(RF_UF_BANC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

RF_UF_BANC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
RF_UF_BANC_Test_ROC <- roc(response = RF_UF_BANC_Test$RF_UF_BANC_Observed,
                           predictor = RF_UF_BANC_Test$RF_UF_BANC_Predicted.Impaired,
                           levels = rev(levels(RF_UF_BANC_Test$RF_UF_BANC_Observed)))

(RF_UF_BANC_Test_ROCCurveAUC <- auc(RF_UF_BANC_Test_ROC)[1])

```


###  1.5.11 Naive Bayes Without UF (NB_FULL)
|
| **[A]** The naive bayes model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented without recursive feature elimination through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 3 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">fL</span> = laplace correction held constant at a value of 0
|      **[B.2]** <span style="color: #FF0000">adjust</span> = bandwidth adjustment held constant at a value of TRUE
|      **[B.3]** <span style="color: #FF0000">usekernel</span> = distribution type made to vary across a range of levels equal to TRUE and FALSE
|
| **[C]** The cross-validated model performance of the final model is summarized as follows:
|      **[C.1]** Final model configuration involves fL=0, adjust=TRUE and usekernel=TRUE
|      **[C.2]** ROC Curve AUC = 0.73904
|
| **[D]** The model does not allow for ranking of predictors in terms of variable importance.
|
| **[E]** The independent test model performance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.68055
|
```{r section_1.5.11, warning=FALSE, message=FALSE}
##################################
# Running the naive bayes model
# by setting the caret method to 'nb'
##################################
set.seed(12345678)
NB_FULL_Tune <- caret::train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                        y = PMA_PreModelling_Train$Class,
                        method = "nb",
                        metric = "ROC",
                        trControl = KFold_TrainControl)

##################################
# Reporting the cross-validation results
# for the train set
##################################
NB_FULL_Tune

NB_FULL_Tune$finalModel

NB_FULL_Tune$results

(NB_FULL_Train_ROCCurveAUC <- NB_FULL_Tune$results[NB_FULL_Tune$results$usekernel==NB_FULL_Tune$bestTune$usekernel,
                                                   c("ROC")])

##################################
# Identifying and plotting the
# best model predictors
##################################
NB_FULL_VarImp <- varImp(NB_FULL_Tune, scale = TRUE)
plot(NB_FULL_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Naive Bayes",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
NB_FULL_Test <- data.frame(NB_FULL_Observed = PMA_PreModelling_Test$Class,
                      NB_FULL_Predicted = predict(NB_FULL_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

##################################
# Reporting the independent evaluation results
# for the test set
##################################
NB_FULL_Test_ROC <- roc(response = NB_FULL_Test$NB_FULL_Observed,
                        predictor = NB_FULL_Test$NB_FULL_Predicted.Impaired,
                        levels = rev(levels(NB_FULL_Test$NB_FULL_Observed)))

(NB_FULL_Test_ROCCurveAUC <- auc(NB_FULL_Test_ROC)[1])

```


###  1.5.12 Naive Bayes With UF Using No Adjustment and Correlated Predictors (NB_UF_NAC)
|
| **[A]** The Naive Bayes model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented with univariate filters using no adjustment for the computed p-values and correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 3 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">fL</span> = laplace correction held constant at a value of 0
|      **[B.2]** <span style="color: #FF0000">adjust</span> = bandwidth adjustment held constant at a value of TRUE
|      **[B.3]** <span style="color: #FF0000">usekernel</span> = distribution type held constant at a value of TRUE
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 58 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 54 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model peNBormance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=47 to 62
|      **[D.2]** ROC Curve AUC = 0.74710
|
| **[E]** The independent test model peNBormance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.69791
|
```{r section_1.5.12, warning=FALSE, message=FALSE}
##################################
# Creating a function to calculates p-values
# using either a t-test for predictors with 
# more than 2 distinct values
# using Fisher's Exact Test otherwise
##################################
PScore <- function(x, y){
    numX <- length(unique(x))
    if(numX > 2)
      {
       out <- t.test(x ~ y)$p.value
      } else {
       out <- fisher.test(factor(x), y)$p.value
      }
    out
}

NBPValue <- nbSBF
NBPValue$score <- PScore
NBPValue$summary <- FiveMetricsSummary

##################################
# Creating a function to filter out
# predictors with p-values greater than 0.05
##################################
NBPValue$filter <- function (Score, x, y){
  InformativePredictors <- Score <= 0.05
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = NBPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the naive bayes model
# by setting the caret method to 'nb'
# with implementation of univariate filter
##################################
set.seed(12345678)
NB_UF_NAC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
NB_UF_NAC_Tune

NB_UF_NAC_Tune$fit

NB_UF_NAC_Tune$results

(NB_UF_NAC_Train_ROCCurveAUC <- NB_UF_NAC_Tune$results[NB_UF_NAC_Tune$results$ROC==max(NB_UF_NAC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
NB_UF_NAC_Test <- data.frame(NB_UF_NAC_Observed = PMA_PreModelling_Test$Class,
                              NB_UF_NAC_Predicted = predict(NB_UF_NAC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

NB_UF_NAC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
NB_UF_NAC_Test_ROC <- roc(response = NB_UF_NAC_Test$NB_UF_NAC_Observed,
                           predictor = NB_UF_NAC_Test$NB_UF_NAC_Predicted.Impaired,
                           levels = rev(levels(NB_UF_NAC_Test$NB_UF_NAC_Observed)))

(NB_UF_NAC_Test_ROCCurveAUC <- auc(NB_UF_NAC_Test_ROC)[1])

```

###  1.5.13 Naive Bayes With UF Using Bonferroni Adjustment and Correlated Predictors (NB_UF_BAC)
|
| **[A]** The Naive Bayes model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented with univariate filters using Bonferroni adjustment for the computed p-values and correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 3 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">fL</span> = laplace correction held constant at a value of 0
|      **[B.2]** <span style="color: #FF0000">adjust</span> = bandwidth adjustment held constant at a value of TRUE
|      **[B.3]** <span style="color: #FF0000">usekernel</span> = distribution type held constant at a value of TRUE
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 15 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 13 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model peNBormance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=10 to 18
|      **[D.2]** ROC Curve AUC = 0.76025
|
| **[E]** The independent test model peNBormance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.76273
|
```{r section_1.5.13, warning=FALSE, message=FALSE}
##################################
# Creating a function to filter out
# predictors with bonferroni-adjusted 
# p-values greater than 0.05
##################################
NBPValue$filter <- function (Score, x, y){
  Score <- p.adjust(Score,  "bonferroni")
  InformativePredictors <- Score <= 0.05
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = NBPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the naive bayes model
# by setting the caret method to 'nb'
# with implementation of univariate filter
##################################
set.seed(12345678)
NB_UF_BAC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
NB_UF_BAC_Tune

NB_UF_BAC_Tune$fit

NB_UF_BAC_Tune$results

(NB_UF_BAC_Train_ROCCurveAUC <- NB_UF_BAC_Tune$results[NB_UF_BAC_Tune$results$ROC==max(NB_UF_BAC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
NB_UF_BAC_Test <- data.frame(NB_UF_BAC_Observed = PMA_PreModelling_Test$Class,
                              NB_UF_BAC_Predicted = predict(NB_UF_BAC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

NB_UF_BAC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
NB_UF_BAC_Test_ROC <- roc(response = NB_UF_BAC_Test$NB_UF_BAC_Observed,
                           predictor = NB_UF_BAC_Test$NB_UF_BAC_Predicted.Impaired,
                           levels = rev(levels(NB_UF_BAC_Test$NB_UF_BAC_Observed)))

(NB_UF_BAC_Test_ROCCurveAUC <- auc(NB_UF_BAC_Test_ROC)[1])

```

###  1.5.14 Naive Bayes With UF Using No Adjustment and No Correlated Predictors (NB_UF_NANC)
|
| **[A]** The Naive Bayes model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented with univariate filters using no adjustment for the computed p-values and no correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 3 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">fL</span> = laplace correction held constant at a value of 0
|      **[B.2]** <span style="color: #FF0000">adjust</span> = bandwidth adjustment held constant at a value of TRUE
|      **[B.3]** <span style="color: #FF0000">usekernel</span> = distribution type held constant at a value of TRUE
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 54 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 52 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model peNBormance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=45 to 59
|      **[D.2]** ROC Curve AUC = 0.74847
|
| **[E]** The independent test model peNBormance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.69907
|
```{r section_1.5.14, warning=FALSE, message=FALSE}
##################################
# Creating a function to filter out
# predictors with p-values greater than 0.05
##################################
NBPValue$filter <- function (Score, x, y){
  InformativePredictors <- Score <= 0.05
  CorrelationMatrix <- cor(x[,InformativePredictors])
  HighlyCorrelated <- findCorrelation(CorrelationMatrix, 0.75)
  if(length(HighlyCorrelated)>0) InformativePredictors[HighlyCorrelated] <- FALSE
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = NBPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the naive bayes model
# by setting the caret method to 'nb'
# with implementation of univariate filter
##################################
set.seed(12345678)
NB_UF_NANC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
NB_UF_NANC_Tune

NB_UF_NANC_Tune$fit

NB_UF_NANC_Tune$results

(NB_UF_NANC_Train_ROCCurveAUC <- NB_UF_NANC_Tune$results[NB_UF_NANC_Tune$results$ROC==max(NB_UF_NANC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
NB_UF_NANC_Test <- data.frame(NB_UF_NANC_Observed = PMA_PreModelling_Test$Class,
                              NB_UF_NANC_Predicted = predict(NB_UF_NANC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

NB_UF_NANC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
NB_UF_NANC_Test_ROC <- roc(response = NB_UF_NANC_Test$NB_UF_NANC_Observed,
                           predictor = NB_UF_NANC_Test$NB_UF_NANC_Predicted.Impaired,
                           levels = rev(levels(NB_UF_NANC_Test$NB_UF_NANC_Observed)))

(NB_UF_NANC_Test_ROCCurveAUC <- auc(NB_UF_NANC_Test_ROC)[1])

```

###  1.5.15 Naive Bayes With UF Using Bonferroni Adjustment and No Correlated Predictors (NB_UF_BANC)
|
| **[A]** The Naive Bayes model from the  <mark style="background-color: #CCECFF">**klaR**</mark> package was implemented with univariate filters using Bonferroni adjustment for the computed p-values and no correlated predictors through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model contains 3 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">fL</span> = laplace correction held constant at a value of 0
|      **[B.2]** <span style="color: #FF0000">adjust</span> = bandwidth adjustment held constant at a value of TRUE
|      **[B.3]** <span style="color: #FF0000">usekernel</span> = distribution type held constant at a value of TRUE
|
| **[C]** Univariate filtering was applied with results as follows:
|      **[C.1]** 15 predictors were selected using the training data.
|      **[C.2]** Resampling showed that approximately 13 variables were selected.
|      **[C.3]** The top 5 variables identified were tied among too many predictors.
|
| **[D]** The cross-validated model peNBormance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves variable subset=10 to 18
|      **[D.2]** ROC Curve AUC = 0.76025
|
| **[E]** The independent test model peNBormance of the final model is summarized as follows:
|      **[E.1]** ROC Curve AUC = 0.76273
|
```{r section_1.5.15, warning=FALSE, message=FALSE}
##################################
# Creating a function to filter out
# predictors with p-values greater than 0.05
##################################
NBPValue$filter <- function (Score, x, y){
  Score <- p.adjust(Score,  "bonferroni")
  InformativePredictors <- Score <= 0.05
  CorrelationMatrix <- cor(x[,InformativePredictors])
  HighlyCorrelated <- findCorrelation(CorrelationMatrix, 0.75)
  if(length(HighlyCorrelated)>0) InformativePredictors[HighlyCorrelated] <- FALSE
  InformativePredictors
}

##################################
# Formulating the controls for the 
# univariate filtering process
##################################
KFold_SBFControl <- sbfControl(method = "cv",
                      verbose = TRUE,
                      functions = NBPValue,
                      index = KFold_Indices,
                      saveDetails = TRUE,
                      returnResamp = "final")

##################################
# Running the naive bayes model
# by setting the caret method to 'nb'
# with implementation of univariate filter
##################################
set.seed(12345678)
NB_UF_BANC_Tune <- caret::sbf(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                              y = PMA_PreModelling_Train$Class,
                              metric = "ROC",
                              sbfControl = KFold_SBFControl)


##################################
# Reporting the cross-validation results
# for the train set
##################################
NB_UF_BANC_Tune

NB_UF_BANC_Tune$fit

NB_UF_BANC_Tune$results

(NB_UF_BANC_Train_ROCCurveAUC <- NB_UF_BANC_Tune$results[NB_UF_BANC_Tune$results$ROC==max(NB_UF_BANC_Tune$results$ROC),
                                                       c("ROC")])

##################################
# Independently evaluating the model and
# reporting the independent evaluation results
# on the test set
##################################
NB_UF_BANC_Test <- data.frame(NB_UF_BANC_Observed = PMA_PreModelling_Test$Class,
                              NB_UF_BANC_Predicted = predict(NB_UF_BANC_Tune,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

NB_UF_BANC_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
NB_UF_BANC_Test_ROC <- roc(response = NB_UF_BANC_Test$NB_UF_BANC_Observed,
                           predictor = NB_UF_BANC_Test$NB_UF_BANC_Predicted.Impaired,
                           levels = rev(levels(NB_UF_BANC_Test$NB_UF_BANC_Observed)))

(NB_UF_BANC_Test_ROCCurveAUC <- auc(NB_UF_BANC_Test_ROC)[1])

```

##  1.6 Model Evaluation Summary
|
| Model performance comparison:
|
| **[A]** Models which applied univariate filters to select a subset of informative predictors performed better than those which utilized the full set of predictors, although the effectiveness varied across the different types of filter methods. 
|      **[A.1]** LDA: Linear Discriminant Analysis (<mark style="background-color: #CCECFF">**MASS**</mark> package)
|             **[A.1.1]** LDA_FULL: Cross-Validation ROC Curve AUC = 0.80151, Test ROC Curve AUC = 0.77199 
|             **[A.1.2]** LDA_UF_NANC: Cross-Validation ROC Curve AUC = 0.84130, Test ROC Curve AUC = 0.88657 
|      **[A.2]** RF: Random Forest (<mark style="background-color: #CCECFF">**randomForest**</mark> package)
|             **[A.2.1]** RF_FULL: Cross-Validation ROC Curve AUC = 0.78268, Test ROC Curve AUC = 0.79803 
|             **[A.2.2]** RF_UF_NANC: Cross-Validation ROC Curve AUC = 0.81301, Test ROC Curve AUC = 0.82002  
|      **[A.3]** NB: Naive Bayes (<mark style="background-color: #CCECFF">**klaR**</mark> package)
|             **[A.3.1]** NB_FULL: Cross-Validation ROC Curve AUC = 0.73904, Test ROC Curve AUC = 0.68055 
|             **[A.3.2]** NB_UF_BAc: Cross-Validation ROC Curve AUC = 0.76825, Test ROC Curve AUC = 0.75231 
|             **[A.3.3]** NB_UF_BANc: Cross-Validation ROC Curve AUC = 0.76025, Test ROC Curve AUC = 0.76273 
|
| **[B]** The models applied with univariate filters which demonstrated the best and most consistent ROC Curve AUC metrics are as follows:
|      **[B.1]**LDA_UF_NANC: Linear Discriminant Analysis With UF Using No Adjustment and No Correlated Predictors (<mark style="background-color: #CCECFF">**MASS**</mark> package)
|
```{r section_1.6, warning=FALSE, message=FALSE}
##################################
# Consolidating all evaluation results
# for the train and test sets
# using the ROC Curve AUC metric
##################################
Model <- c('LDA_FULL','LDA_UF_NAC','LDA_UF_BAC','LDA_UF_NANC','LDA_UF_BANC',
           'RF_FULL','RF_UF_NAC','RF_UF_BAC','RF_UF_NANC','RF_UF_BANC',
           'NB_FULL','NB_UF_NAC','NB_UF_BAC','NB_UF_NANC','NB_UF_BANC',
           'LDA_FULL','LDA_UF_NAC','LDA_UF_BAC','LDA_UF_NANC','LDA_UF_BANC',
           'RF_FULL','RF_UF_NAC','RF_UF_BAC','RF_UF_NANC','RF_UF_BANC',
           'NB_FULL','NB_UF_NAC','NB_UF_BAC','NB_UF_NANC','NB_UF_BANC')

Set <- c(rep('Cross-Validation',15),rep('Test',15))

ROCCurveAUC <- c(LDA_FULL_Train_ROCCurveAUC,
                 LDA_UF_NAC_Train_ROCCurveAUC,
                 LDA_UF_BAC_Train_ROCCurveAUC,
                 LDA_UF_NANC_Train_ROCCurveAUC,
                 LDA_UF_BANC_Train_ROCCurveAUC,
                 RF_FULL_Train_ROCCurveAUC,
                 RF_UF_NAC_Train_ROCCurveAUC,
                 RF_UF_BAC_Train_ROCCurveAUC,
                 RF_UF_NANC_Train_ROCCurveAUC,
                 RF_UF_BANC_Train_ROCCurveAUC,
                 NB_FULL_Train_ROCCurveAUC,
                 NB_UF_NAC_Train_ROCCurveAUC,
                 NB_UF_BAC_Train_ROCCurveAUC,
                 NB_UF_NANC_Train_ROCCurveAUC,
                 NB_UF_BANC_Train_ROCCurveAUC,
                 LDA_FULL_Test_ROCCurveAUC,
                 LDA_UF_NAC_Test_ROCCurveAUC,
                 LDA_UF_BAC_Test_ROCCurveAUC,
                 LDA_UF_NANC_Test_ROCCurveAUC,
                 LDA_UF_BANC_Test_ROCCurveAUC,
                 RF_FULL_Test_ROCCurveAUC,
                 RF_UF_NAC_Test_ROCCurveAUC,
                 RF_UF_BAC_Test_ROCCurveAUC,
                 RF_UF_NANC_Test_ROCCurveAUC,
                 RF_UF_BANC_Test_ROCCurveAUC,
                 NB_FULL_Test_ROCCurveAUC,
                 NB_UF_NAC_Test_ROCCurveAUC,
                 NB_UF_BAC_Test_ROCCurveAUC,
                 NB_UF_NANC_Test_ROCCurveAUC,
                 NB_UF_BANC_Test_ROCCurveAUC)

ROCCurveAUC_Summary <- as.data.frame(cbind(Model,Set,ROCCurveAUC))

ROCCurveAUC_Summary$ROCCurveAUC <- as.numeric(as.character(ROCCurveAUC_Summary$ROCCurveAUC))
ROCCurveAUC_Summary$Set <- factor(ROCCurveAUC_Summary$Set,
                                        levels = c("Cross-Validation",
                                                   "Test"))
ROCCurveAUC_Summary$Model <- factor(ROCCurveAUC_Summary$Model,
                                        levels = c('LDA_FULL',
                                                   'LDA_UF_NAC',
                                                   'LDA_UF_BAC',
                                                   'LDA_UF_NANC',
                                                   'LDA_UF_BANC',
                                                   'RF_FULL',
                                                   'RF_UF_NAC',
                                                   'RF_UF_BAC',
                                                   'RF_UF_NANC',
                                                   'RF_UF_BANC',
                                                   'NB_FULL',
                                                   'NB_UF_NAC',
                                                   'NB_UF_BAC',
                                                   'NB_UF_NANC',
                                                   'NB_UF_BANC'))

print(ROCCurveAUC_Summary, row.names=FALSE)

(ROCCurveAUC_Plot <- dotplot(Model ~ ROCCurveAUC,
                           data = ROCCurveAUC_Summary,
                           groups = Set,
                           main = "Classification Model Performance Comparison",
                           ylab = "Model",
                           xlab = "ROC Curve AUC",
                           auto.key = list(adj = 1),
                           type=c("p", "h"),
                           origin = 0,
                           alpha = 0.45,
                           pch = 16,
                           cex = 2))
```

##  1.7 References
|
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [rpart.plot](https://cran.r-project.org/web/packages/rpart.plot/rpart.plot.pdf) by Stephen Milborrow
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [pls](https://cran.r-project.org/web/packages/pls/pls.pdf) by Kristian Hovde Liland
| **[R Package]** [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) by Brian Ripley
| **[R Package]** [elasticnet](https://cran.r-project.org/web/packages/elasticnet/elasticnet.pdf) by Hui Zou
| **[R Package]** [earth](https://cran.r-project.org/web/packages/earth/earth.pdf) by Stephen Milborrow
| **[R Package]** [party](https://cran.r-project.org/web/packages/party/party.pdf) by Torsten Hothorn
| **[R Package]** [kernlab](https://cran.r-project.org/web/packages/kernlab/kernlab.pdf) by Alexandros Karatzoglou
| **[R Package]** [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) by Andy Liaw
| **[R Package]** [pROC](https://cran.r-project.org/web/packages/pROC/pROC.pdf) by Xavier Robin
| **[R Package]** [mda](https://cran.r-project.org/web/packages/mda/mda.pdf) by Trevor Hastie
| **[R Package]** [klaR](https://cran.r-project.org/web/packages/klaR/klaR.pdf) by Christian Roever, Nils Raabe, Karsten Luebke, Uwe Ligges, Gero Szepannek, Marc Zentgraf and David Meyer
| **[R Package]** [pamr](https://cran.r-project.org/web/packages/pamr/pamr.pdf) by Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan and Gil Chu
| **[Article]** [The caret Package](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[Article]** [A Short Introduction to the caret Package](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) by Max Kuhn
| **[Article]** [Caret Package – A Practical Guide to Machine Learning in R](https://www.machinelearningplus.com/machine-learning/caret-package/#:~:text=Caret%20is%20short%20for%20Classification%20And%20REgression%20Training.,track%20of%20which%20algorithm%20resides%20in%20which%20package.) by Selva Prabhakaran
| **[Article]** [Tuning Machine Learning Models Using the Caret R Package](https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/) by Jason Brownlee
| **[Article]** [Lattice Graphs](http://www.sthda.com/english/wiki/lattice-graphs) by Alboukadel Kassambara
| **[Article]** [A Tour of Machine Learning Algorithms](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/) by Jason Brownlee
| **[Article]** [How to Choose a Feature Selection Method For Machine Learning](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/) by Jason Brownlee
| **[Article]** [Filter Based Feature Selection](https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/filter-based-feature-selection) by Microsoft Azure Team
| **[Article]** [Feature Selection: Filter method, Wrapper method and Embedded Method](https://www.izen.ai/blog-posts/feature-selection-filter-method-wrapper-method-and-embedded-method/) by Shripad Bhat
| **[Article]** [Demystifying Feature Selection: Filter vs Wrapper Methods](https://www.explorium.ai/blog/demystifying-feature-selection-filter-vs-wrapper-methods/) by Explorium Team
| **[Article]** [Decision Tree Algorithm Examples In Data Mining](https://www.softwaretestinghelp.com/decision-tree-algorithm-examples-data-mining/) by Software Testing Help Team
| **[Article]** [4 Types of Classification Tasks in Machine Learning](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) by Jason Brownlee
| **[Article]** [Spot-Check Classification Machine Learning Algorithms in Python with scikit-learn](https://machinelearningmastery.com/spot-check-classification-machine-learning-algorithms-python-scikit-learn/) by Jason Brownlee
| **[Article]** [An Introduction to Naive Bayes Algorithm for Beginners](https://www.turing.com/kb/an-introduction-to-naive-bayes-algorithm-for-beginners) by Turing Team
| **[Article]** [Machine Learning Tutorial: A Step-by-Step Guide for Beginners](http://www.feat.engineering/index.html) by Mayank Banoula
| **[Article]** [Discriminant Analysis Essentials in R](http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/) by Alboukadel Kassambara
| **[Article]** [Linear Discriminant Analysis, Explained](https://yangxiaozhou.github.io/data/2019/10/02/linear-discriminant-analysis.html) by Xiaozhou Yang
| **[Article]** [Classification Tree](https://support.bccvl.org.au/support/solutions/articles/6000083204-classification-tree) by BCCVL Team
| **[Article]** [Random Forest](https://support.bccvl.org.au/support/solutions/articles/6000083217-random-forest) by BCCVL Team
| **[Article]** [Generalized Linear Model](https://support.bccvl.org.au/support/solutions/articles/6000083213-generalized-linear-model) by BCCVL Team
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
| **[Course]** [Regression Methods](https://online.stat.psu.edu/stat501/) by Penn State Eberly College of Science
| **[Course]** [Applied Regression Analysis](https://online.stat.psu.edu/stat462/) by Penn State Eberly College of Science
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|